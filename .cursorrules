# Autocost Controller - Cursor Rules

## Project Overview
This is a **production-ready multi-cloud cost optimization platform** with **MCP (Model Context Protocol)** integration. The project follows a modular, provider-based architecture supporting **AWS, GCP, Azure, and DataDog** with a streamlined all-in-one setup experience.

## Core Architecture Principles

### 1. Provider-Based Architecture
- Each cloud provider has its own module under `autocost_controller/providers/{provider_name}/`
- Providers must inherit from `BaseProvider` in `autocost_controller/core/provider_manager.py`
- All providers implement the same interface: `validate_configuration()`, `test_connection()`, `get_capabilities()`
- **DataDog provider** is fully implemented with authentication, client, models, and tools

### 2. Unified Entry Point
- **`start.py`** is the single entry point for all operations
- Supports multiple modes: default (quick setup), `--configure`, `--server`, `--verify`
- Integrates credential capture, dependency management, and setup verification
- **NO OTHER SCRIPTS** should be in the root directory

### 3. Environment-Based Configuration
- Use environment variables for provider selection: `AUTOCOST_PROVIDERS=aws,datadog`
- Endpoint identification via `AUTOCOST_ENDPOINT=aws` or `unified`
- Additive `.env` file management - preserves existing settings
- **Automatic credential capture** from current session/environment

### 4. MCP Tool Organization
- Tools are organized by provider in `autocost_controller/tools/{provider}_tools.py`
- Tool names follow the pattern: `{provider}_{category}_{action}`
- Examples: `aws_cost_explorer_analyze_costs`, `datadog_logs_search`, `aws_profile_switch`

## Coding Standards

### Python Style
- Follow PEP 8 with line length of 100 characters
- Use type hints for all function parameters and return values
- Use docstrings for all classes, methods, and functions
- Prefer f-strings over .format() or % formatting

### Import Organization
```python
# Standard library imports
import asyncio
import sys
from pathlib import Path
from typing import Dict, List, Optional

# Third-party imports
from mcp.server.fastmcp import FastMCP
from rich.console import Console

# Local imports
from autocost_controller.core.config import Config
from autocost_controller.core.logger import AutocostLogger
```

### Error Handling
- Always use specific exception types when possible
- Log errors with provider context: `logger.error("message", provider="aws")`
- Use try-except blocks for external API calls
- Return meaningful error messages to users

### Logging Standards
```python
# Use the AutocostLogger with appropriate methods
logger.info("Operation completed successfully")
logger.provider_status("aws", "ready", "All services available")
logger.error("Failed to connect to API", provider="aws")
logger.cost_analysis_summary("aws", 1234.56, 7, 5)
```

## Provider Implementation Guidelines

### Creating a New Provider

1. **Directory Structure**
```
autocost_controller/providers/{provider_name}/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ provider.py          # Main provider class
‚îú‚îÄ‚îÄ client.py           # API client wrapper
‚îú‚îÄ‚îÄ models.py           # Provider-specific data models
‚îî‚îÄ‚îÄ auth.py             # Authentication handling
```

2. **Provider Class Template**
```python
from typing import List
from ...core.provider_manager import BaseProvider
from ...core.models import ProviderType, ProviderStatus

class {ProviderName}Provider(BaseProvider):
    def get_provider_name(self) -> ProviderType:
        return "{provider_name}"
    
    def validate_configuration(self) -> ProviderStatus:
        # Implement configuration validation
        pass
    
    def get_capabilities(self) -> List[str]:
        return [
            "cost_analysis",
            "performance_metrics", 
            "optimization_recommendations"
        ]
    
    async def test_connection(self) -> bool:
        # Implement connection testing
        pass
```

3. **Tool Registration Pattern**
```python
def register_{provider}_tools(mcp: FastMCP, provider_manager: ProviderManager, 
                             config: Config, logger: AutocostLogger) -> None:
    """Register {provider} cost analysis tools."""
    
    {provider}_provider = provider_manager.get_provider("{provider}")
    if not {provider}_provider:
        logger.warning("_{provider} provider not available, skipping tools registration")
        return
    
    @mcp.tool()
    async def {provider}_cost_explorer_analyze_costs(
        days: int = 7,
        # other parameters...
    ) -> str:
        """{Provider} Cost Explorer: Detailed cost analysis with breakdowns."""
        logger.info(f"üîç Analyzing {provider} costs for {days} days...")
        # Implementation here
```

### Tool Naming Convention
- Format: `{provider}_{category}_{action}`
- Provider: `aws`, `gcp`, `azure`, `datadog`
- Category: `cost_explorer`, `performance`, `billing`, `usage`, `logs`, `metrics`, `profile`
- Action: `analyze_costs`, `get_insights`, `list_services`, `search`, `query`, `switch`

Examples:
- ‚úÖ `aws_cost_explorer_analyze_costs` - AWS cost analysis
- ‚úÖ `aws_profile_switch` - AWS profile management
- ‚úÖ `datadog_logs_search` - DataDog log search
- ‚úÖ `datadog_metrics_query` - DataDog metrics query
- ‚úÖ `datadog_usage_analysis` - DataDog usage analysis
- ‚ùå `get_aws_costs` (wrong format)
- ‚ùå `analyze_gcp_billing_data` (inconsistent)

## Configuration Standards

### Environment Variables
- Use `AUTOCOST_` prefix for all project-specific variables
- Provider lists: `AUTOCOST_PROVIDERS=aws,datadog`
- Endpoint identification: `AUTOCOST_ENDPOINT=unified`
- DataDog: `DATADOG_API_KEY`, `DATADOG_APP_KEY`, `DATADOG_SITE`
- AWS: Automatically captured from session/environment

### Setup and Configuration
- **Primary method**: Use `python start.py` for all setup operations
- **Additive configuration**: Preserves existing `.env` settings
- **Automatic dependency management**: Installs all required packages
- **Credential capture**: Automatically detects and saves working credentials

### Config Class Usage
```python
config = Config()
enabled_providers = config.enabled_providers  # From environment
aws_region = config.aws.default_region
log_level = config.logging.level
```

## Testing Standards

### Test Structure
- Unit tests in `tests/unit/`
- Integration tests in `tests/integration/`
- Provider-specific tests in `tests/providers/{provider}/`
- Use pytest with async support

### Test Naming
```python
def test_aws_provider_validates_configuration():
    """Test AWS provider configuration validation."""

async def test_aws_cost_analysis_returns_valid_data():
    """Test AWS cost analysis tool returns properly formatted data."""
```

## Documentation Standards

### Docstring Format
```python
def analyze_costs(
    provider: str,
    days: int = 7,
    include_tags: bool = True
) -> Dict[str, Any]:
    """
    Analyze cloud costs for specified time period.
    
    Args:
        provider: Cloud provider name (aws, gcp, azure)
        days: Number of days to analyze (default: 7)
        include_tags: Whether to include tag-based analysis
        
    Returns:
        Dict containing cost breakdown and analysis results
        
    Raises:
        ProviderError: If provider is not available or configured
        ValidationError: If parameters are invalid
        
    Example:
        >>> results = analyze_costs("aws", days=30, include_tags=True)
        >>> print(results["total_cost"])
        1234.56
    """
```

### README Updates
- Keep README.md current with new features
- Include example usage for new tools
- Update configuration examples
- Document new environment variables

## Security Guidelines

### Credential Management
- **NEVER hardcode credentials** in source code
- **Automatic credential capture**: `start.py` captures working credentials from environment
- **AWS session support**: Handles temporary credentials and assumed roles
- **Environment variables**: Store DataDog API keys and other credentials
- **Additive configuration**: Preserves existing settings when updating
- **Connection testing**: Validates credentials before saving
- Log authentication attempts (success/failure) without exposing secrets

### SSL Certificate Handling (Corporate Environments)
- **ALWAYS implement SSL bypass** for DataDog API calls in corporate environments
- **Pattern**: Wrap all external API calls with SSL verification fallback
- **Required for**: DataDog dashboards, metrics, logs, usage APIs
- **Implementation**: Use `reinitialize_with_ssl_disabled()` method on SSL errors

```python
# ‚úÖ Correct SSL handling pattern for DataDog API calls
try:
    response = self.api_client.some_api_call(params)
except Exception as ssl_error:
    if "SSL: CERTIFICATE_VERIFY_FAILED" in str(ssl_error):
        self.logger.info("SSL verification failed, reinitializing with SSL disabled", "datadog")
        self.reinitialize_with_ssl_disabled()
        response = self.api_client.some_api_call(params)
    else:
        raise ssl_error
```

- **Disable SSL warnings**: Use `urllib3.disable_warnings()` when SSL verification is disabled
- **Log SSL fallback**: Always log when falling back to disabled SSL verification
- **Corporate detection**: Automatic detection and handling without user intervention

### API Key Handling
```python
# ‚úÖ Good
api_key = os.environ.get("GCP_API_KEY")
if not api_key:
    raise ConfigurationError("GCP_API_KEY environment variable required")

# ‚ùå Bad
api_key = "your-actual-api-key-here"
```

### Logging Security
```python
# ‚úÖ Good
logger.info(f"Authenticated to {provider} for account {account_id[:4]}****")

# ‚ùå Bad  
logger.info(f"Using API key: {api_key}")
```

## Performance Guidelines

### Async/Await Usage
- Use async/await for all I/O operations
- Implement proper error handling in async functions
- Use asyncio.gather() for concurrent operations

### Caching Strategy
```python
# Cache expensive operations
@lru_cache(maxsize=128, ttl=300)  # 5-minute cache
async def get_cost_data(provider: str, date_range: str) -> Dict:
    """Cache cost data to reduce API calls."""
```

### API Rate Limiting
- Implement backoff strategies for API calls
- Respect provider rate limits
- Use connection pooling where appropriate

## Start.py Command Interface

### Command-Line Modes
```bash
python start.py                    # Default: Quick setup (capture credentials + verify)
python start.py --configure        # Full configuration wizard
python start.py --server           # Start the MCP server
python start.py --verify           # Verify current setup
python start.py --help             # Show usage information
```

### Mode-Specific Behavior
- **Default mode**: Automatic credential capture, dependency check, setup verification
- **Configure mode**: Full interactive setup with provider configuration and IDE integration
- **Server mode**: Pre-verification then server startup
- **Verify mode**: Non-destructive setup validation with detailed reporting

## File Organization

### Root Directory (Clean)
```
‚îú‚îÄ‚îÄ start.py                    # Single entry point for all operations  
‚îú‚îÄ‚îÄ README.md                   # Comprehensive documentation
‚îú‚îÄ‚îÄ PROVIDER_DEVELOPMENT.md     # Provider development guide
‚îú‚îÄ‚îÄ pyproject.toml              # Dependencies and project metadata
‚îî‚îÄ‚îÄ .env                        # Environment variables (auto-managed)
```

### Module Structure
```
autocost_controller/
‚îú‚îÄ‚îÄ core/                    # Core functionality
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ logger.py           # Enhanced logging
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Data models
‚îÇ   ‚îî‚îÄ‚îÄ provider_manager.py # Provider lifecycle
‚îú‚îÄ‚îÄ providers/              # Provider implementations
‚îÇ   ‚îú‚îÄ‚îÄ aws/               # AWS provider (complete)
‚îÇ   ‚îú‚îÄ‚îÄ gcp/               # Google Cloud provider (structure only)
‚îÇ   ‚îú‚îÄ‚îÄ azure/             # Azure provider (structure only)
‚îÇ   ‚îî‚îÄ‚îÄ datadog/           # DataDog provider (complete)
‚îÇ       ‚îú‚îÄ‚îÄ auth.py        # Authentication handling
‚îÇ       ‚îú‚îÄ‚îÄ client.py      # API client wrapper
‚îÇ       ‚îú‚îÄ‚îÄ models.py      # Data models
‚îÇ       ‚îî‚îÄ‚îÄ provider.py    # Provider implementation
‚îú‚îÄ‚îÄ tools/                  # MCP tool implementations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py        # Core tools registration
‚îÇ   ‚îú‚îÄ‚îÄ aws_tools.py       # AWS-specific tools
‚îÇ   ‚îî‚îÄ‚îÄ datadog_tools.py   # DataDog-specific tools
‚îî‚îÄ‚îÄ utils/                  # Shared utilities
```

### File Naming
- Use snake_case for Python files: `aws_tools.py`, `provider_manager.py`
- Use kebab-case for documentation: `setup-guide.md`, `provider-development.md`
- **Root directory**: Only `start.py` and essential files (no utility scripts)

## Git Workflow

### Commit Messages
```
feat(aws): add EC2 right-sizing recommendations
fix(gcp): resolve billing API authentication error
docs(readme): update provider configuration examples
refactor(core): simplify provider initialization logic
```

### Branch Naming
- `feature/aws-reserved-instances`
- `fix/gcp-auth-timeout`
- `docs/provider-development-guide`
- `refactor/simplify-configuration`

### Pull Request Guidelines
1. Include tests for new functionality
2. Update documentation for user-facing changes
3. Follow the provider implementation checklist
4. Test with multiple providers if applicable

## Tools and IDE Integration

### Recommended Extensions
- Python extension with type checking
- Black formatter
- isort for import organization
- Ruff for linting
- Better docstring support

### VS Code Settings
```json
{
    "python.formatting.provider": "black",
    "python.linting.enabled": true,
    "python.linting.ruffEnabled": true,
    "python.defaultInterpreterPath": "./.venv/bin/python"
}
```

## Error Patterns to Avoid

### Common Mistakes
```python
# ‚ùå Avoid hardcoded values
aws_role = "arn:aws:iam::123456789012:role/CostAnalysis"

# ‚úÖ Use configuration
aws_role = config.aws.analysis_role_arn

# ‚ùå Avoid provider-specific logic in core
if provider == "aws":
    # AWS-specific code in core module

# ‚úÖ Use provider abstraction
cost_data = await provider.get_cost_data(params)

# ‚ùå Avoid exposing implementation details
return {"aws_client": client, "raw_response": response}

# ‚úÖ Return clean interfaces
return CostAnalysisResult(total=1234.56, breakdown=breakdown)

# ‚ùå DataDog API calls without SSL handling
response = self.dashboards_api.get_dashboard(dashboard_id)

# ‚úÖ DataDog API calls with SSL handling
try:
    response = self.dashboards_api.get_dashboard(dashboard_id)
except Exception as ssl_error:
    if "SSL: CERTIFICATE_VERIFY_FAILED" in str(ssl_error):
        self.reinitialize_with_ssl_disabled()
        response = self.dashboards_api.get_dashboard(dashboard_id)
    else:
        raise ssl_error
```

### Performance Anti-patterns
```python
# ‚ùå Sequential API calls
for account in accounts:
    costs = await get_account_costs(account)

# ‚úÖ Concurrent API calls
cost_tasks = [get_account_costs(account) for account in accounts]
costs = await asyncio.gather(*cost_tasks)
```

## Latest Developments (2024)

### DataDog Provider Implementation
- **Complete provider**: Authentication, client, models, tools
- **API integration**: Logs, metrics, dashboards, usage analysis
- **Connection testing**: Validates API/App keys before saving
- **Tools**: `datadog_logs_search`, `datadog_metrics_query`, `datadog_usage_analysis`

### Enhanced Setup Experience
- **One-command setup**: `python start.py` handles everything
- **Automatic dependency management**: Installs from pyproject.toml
- **Credential capture**: Detects and saves working AWS credentials
- **Multi-mode operation**: Setup, server, verification modes
- **IDE integration**: Auto-configures Claude Desktop and Cursor

### Credential Management Improvements
- **Session token support**: Handles AWS assumed roles automatically
- **Environment detection**: Finds working credentials from environment
- **Additive configuration**: Updates .env without destroying existing settings
- **Connection validation**: Tests AWS and DataDog connections before saving
- **SSL Certificate Workaround**: All DataDog API calls must handle SSL verification failures for corporate environments

### Project Structure Cleanup
- **Single entry point**: Only `start.py` in root directory
- **Consolidated documentation**: Comprehensive README with all info
- **Streamlined workflow**: Setup ‚Üí Verify ‚Üí Run server

## Review Checklist

Before submitting code:
- [ ] All functions have type hints and docstrings
- [ ] New providers follow the BaseProvider interface
- [ ] Tool names follow the naming convention
- [ ] Error handling includes provider context
- [ ] Tests cover new functionality
- [ ] Documentation is updated
- [ ] No hardcoded credentials or secrets
- [ ] Environment variables are documented
- [ ] Async/await used for I/O operations
- [ ] Logging follows the project standards
- [ ] **New**: Only essential files in root directory (no utility scripts)
- [ ] **New**: Use start.py for all setup and operational tasks
- [ ] **Critical**: All DataDog API calls implement SSL certificate workaround pattern
- [ ] **Critical**: SSL verification fallback tested in corporate environments

## Getting Help

- Check existing provider implementations for patterns
- Refer to the BaseProvider interface for required methods
- Use the AutocostLogger for consistent logging
- Follow the tool registration patterns in existing files
- Test with `python start.py --test` before submitting 